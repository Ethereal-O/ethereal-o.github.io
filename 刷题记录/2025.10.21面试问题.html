<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Ethereal">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/刷题记录/2025.10.21面试问题.html"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="一、 实习经历 (Internship Experience)1. 大岩资本 (Jasper Capital)面试官提问：  你对大岩资本的这段量化开发实习有什么理解？  你在简历中提到，将部分机器学习Python程序以及sklearn等库函数转变为高性能的C++程序 。你能否举一个具体的例子，说明你做了哪些维度的优化？是算法层面的，还是工程层面的（如内存管理、SIMD指令）？  你参与了深度学习">
<meta property="og:type" content="website">
<meta property="og:title" content="251021面试问题">
<meta property="og:url" content="http://example.com/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/2025.10.21%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="一、 实习经历 (Internship Experience)1. 大岩资本 (Jasper Capital)面试官提问：  你对大岩资本的这段量化开发实习有什么理解？  你在简历中提到，将部分机器学习Python程序以及sklearn等库函数转变为高性能的C++程序 。你能否举一个具体的例子，说明你做了哪些维度的优化？是算法层面的，还是工程层面的（如内存管理、SIMD指令）？  你参与了深度学习">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-10-21T13:33:00.000Z">
<meta property="article:modified_time" content="2025-10-30T14:10:37.473Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            251021面试问题 -
        
        Ethereal&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"en"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"Welcome to Ethereal's Blog","subtitle":{"text":["A willing horse needs no spur."],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.2.1","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Github":{"path":"https://github.com/Ethereal-O/","icon":"fa-brands fa-github"},"CSDN":{"path":"https://blog.csdn.net/weixin_51969975","icon":"fa-brands fa-stack-overflow"},"Links":{"icon":"fa-regular fa-link","submenus":{"Fontawesome":"https://fontawesome.com/search","Iconfont":"https://www.iconfont.cn/","Redefine":"https://redefine-docs.ohevan.com/introduction","Linyu":"https://www.linyu.cool/","Electronic-Waste":"https://blog.electronicwaste.cn/?","Thysrael":"https://thysrael.github.io/?","World-explorer":"https://www.cnblogs.com/world-explorer"}}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2023/8/1 00:00:00"};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Ethereal&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    target="_blank" rel="noopener" href="https://github.com/Ethereal-O/"  >
                                    
                                        
                                            <i class="fa-brands fa-github"></i>
                                        
                                        GITHUB
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51969975"  >
                                    
                                        
                                            <i class="fa-brands fa-stack-overflow"></i>
                                        
                                        CSDN
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-link"></i>
                                        
                                        LINKS&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://fontawesome.com/search">FONTAWESOME
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://www.iconfont.cn/">ICONFONT
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://redefine-docs.ohevan.com/introduction">REDEFINE
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://www.linyu.cool/">LINYU
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://blog.electronicwaste.cn/?">ELECTRONIC-WASTE
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://thysrael.github.io/?">THYSRAEL
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://www.cnblogs.com/world-explorer">WORLD-EXPLORER
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        target="_blank" rel="noopener" href="https://github.com/Ethereal-O/"  >
                             
                                
                                    <i class="fa-brands fa-github"></i>
                                
                                GITHUB
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51969975"  >
                             
                                
                                    <i class="fa-brands fa-stack-overflow"></i>
                                
                                CSDN
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-link"></i>
                                
                                LINKS&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://fontawesome.com/search">FONTAWESOME</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://www.iconfont.cn/">ICONFONT</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://redefine-docs.ohevan.com/introduction">REDEFINE</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://www.linyu.cool/">LINYU</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://blog.electronicwaste.cn/?">ELECTRONIC-WASTE</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://thysrael.github.io/?">THYSRAEL</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://www.cnblogs.com/world-explorer">WORLD-EXPLORER</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="page-template-container">
        
        
        
        <div class="page-template-content markdown-body">
            
                <h3 id="一、-实习经历-Internship-Experience"><a href="#一、-实习经历-Internship-Experience" class="headerlink" title="一、 实习经历 (Internship Experience)"></a>一、 实习经历 (Internship Experience)</h3><h4 id="1-大岩资本-Jasper-Capital"><a href="#1-大岩资本-Jasper-Capital" class="headerlink" title="1. 大岩资本 (Jasper Capital)"></a>1. 大岩资本 (Jasper Capital)</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>你对大岩资本的这段量化开发实习有什么理解？</p>
</li>
<li><p>你在简历中提到，将部分机器学习Python程序以及sklearn等库函数转变为高性能的C++程序 。你能否举一个具体的例子，说明你做了哪些维度的优化？是算法层面的，还是工程层面的（如内存管理、SIMD指令）？</p>
</li>
<li><p>你参与了深度学习平台的开发，部署了高可用的airflow、argo、ceph、k8s、kubeflow集群 。在确保这套复杂系统“高可用”时，你遇到的最大挑战是什么？你提到的“分层硬件提升写入速率40%” 是如何实现的？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（对实习的理解）</strong> 在我看来，大岩资本的这段实习 是我将<strong>底层系统知识</strong>与<strong>上层AI&#x2F;ML应用</strong>相结合的宝贵实践。我深刻理解到量化金融领域对<strong>性能</strong>和<strong>稳定性</strong>的极致追求。</p>
<ol>
<li><p><strong>一方面是稳定性（高可用）</strong>：量化研究和交易依赖一个强大的深度学习平台 。我通过参与部署K8s、Kubeflow、Ceph等高可用集群 ，确保了研究员的工作流（如Airflow）和数据存储（Ceph）不会因为单点故障而中断。</p>
</li>
<li><p><strong>另一方面是高性能</strong>：在量化领域，毫秒甚至微秒级的延迟都可能影响策略表现。我负责的“Python转C++”工作并非简单的代码翻译，而是对性能的极致压榨，这让我对高性能计算有了更深的理解。同时，通过K8s管理GPU集群和优化硬件，也让我学会了如何从基础设施层面为上层应用提效。</p>
</li>
<li><p>根据技术面还是HR面，改变答案是偏向技术还是偏向量化&#x2F;互联网区别</p>
</li>
</ol>
</li>
<li><p><strong>（Python转C++优化）</strong> 我处理过的一个典型例子是将<code>sklearn</code>的计算互信息的Python库转换为高性能C++ 。</p>
<ol>
<li><p><strong>瓶颈分析</strong>：Python版本的瓶颈主要在于：(1) Sklearn虽然底层是C，但在数据流转和Python的胶水层有开销；(2) Python的GIL导致无法充分利用多核。</p>
</li>
<li><p><strong>优化行动</strong>：</p>
<ul>
<li><p><strong>算法实现</strong>：我没有直接复用<code>sklearn</code>的库，而是用C++重新实现了核心的数值计算逻辑。</p>
</li>
<li><p><strong>并行化</strong>：对于可以并行的计算（例如对不同样本的同一个特征进行处理），我使用了<code>std::aync</code>进行多线程并行化。</p>
</li>
<li><p><strong>nanoflann优化</strong>：在最核心的计算KDTree算法中，我使用perf分析了瓶颈，发现其在仅仅需要获取周围邻居数目的接口中，花费大量时间计算出了具体值，而这是没有必要的。因此我重写了算法，使得在大量数据的情况下提速超过50%。</p>
</li>
</ul>
</li>
<li><p><strong>结果</strong>：通过这些优化，C++程序的性能相比原始Python版本提升了约50%，显著降低了策略执行的延迟。</p>
</li>
</ol>
</li>
<li><p><strong>（高可用挑战与分层硬件）</strong></p>
<ol>
<li><p><strong>最大挑战</strong>：部署这套MALL（Machine Learning）平台时，最大的挑战在于<strong>有状态服务（Stateful Services）的高可用和性能的权衡</strong>。例如，<code>PostgreSQL</code>（作为Airflow和Kubeflow的元数据库）和<code>Ceph</code>（作为存储后端）的数据一致性和故障自动转移（Failover）是关键。</p>
</li>
<li><p>为什么是挑战？</p>
<ul>
<li><p><strong>Kubernetes (K8s) 本身是面向无状态服务的：</strong> K8s 擅长管理 Airflow Worker、Argo Controller 等无状态应用（可以随时销毁和重建），但对需要持久化存储并维护状态的服务（如数据库、分布式存储系统）的支持和管理更为复杂。</p>
</li>
<li><p><strong>PostgreSQL 的数据一致性要求极高：</strong> PostgreSQL 作为 Airflow 和 Kubeflow 的<strong>元数据库</strong>，存储着关键的调度信息和实验追踪数据。一旦发生故障，必须确保<strong>主从切换 (Failover) 过程中数据不丢失、不损坏，且切换时间极短</strong>，否则整个调度和实验平台会瘫痪。</p>
</li>
<li><p><strong>Ceph 的高可用性和性能权衡：</strong> Ceph 作为<strong>核心存储后端</strong>，其高可用依赖于多副本策略和复杂的网络环境。在追求高可用性的同时，如何保证在深度学习<strong>高并发、大吞吐量的写入场景</strong>下，性能不成为瓶颈，是一个巨大的挑战。数据丢失或写入延迟高，会直接导致训练任务失败或效率低下。</p>
</li>
</ul>
</li>
<li><p>解决方案：分层高可用架构</p>
<ul>
<li><p><strong>核心集群层：</strong> Kubernetes (K8s) 集群自身通过部署多个 Controller Node 来确保高可用性，核心的 etcd 集群也采用了多副本部署。</p>
</li>
<li><p><strong>存储层 (Ceph)：</strong> Ceph 作为独立服务运行，利用多副本 (Replica) 策略和合理的 CRUSH Map 配置，保证了底层数据的<strong>高冗余和高可用性</strong>。</p>
</li>
<li><p><strong>关键状态服务层 (PostgreSQL)：</strong> PostgreSQL 通过部署<strong>主从复制集群</strong>实现自动故障切换以保证高可用，并使用 <strong>PgBouncer</strong> 管理连接池，以应对上层应用的大量并发连接。</p>
</li>
<li><p><strong>上层应用层：</strong> Airflow 和 Argo 均部署在 K8s 上，并利用部署在 K8s 上的 <strong>RabbitMQ</strong> 处理消息队列，进一步解耦和确保了系统的健壮性。</p>
</li>
</ul>
</li>
<li><p><strong>分层硬件提速40%<strong>：这是针对<code>Ceph</code> 写入性能的优化。我们的<code>Ceph</code>集群 采用了</strong>分层硬件</strong> 架构：</p>
<ul>
<li><p><strong>Journal&#x2F;WAL层</strong>：我们使用了高性能的NVMe SSD作为Ceph OSD的日志（Journal）或WAL（Write-Ahead Log）。所有的写入请求会先被极快地写入到SSD中并返回确认，这一步延迟极低。</p>
</li>
<li><p><strong>数据层</strong>：后台进程（Flusher）再异步地将数据从SSD刷（Flush）到成本更低、容量更大的HDD（机械硬盘）上进行持久化。</p>
</li>
<li><p>这种设计充分利用了SSD的低延迟和HDD的高容量，使得集群的整体写入速率获得了约40%的提升 ，这对深度学习训练时频繁写入Checkpoint和日志的场景至关重要。</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<hr>
<h4 id="2-百度-Baidu-开源之夏"><a href="#2-百度-Baidu-开源之夏" class="headerlink" title="2. 百度 (Baidu - 开源之夏)"></a>2. 百度 (Baidu - 开源之夏)</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>你对百度开源之夏的这段开发实习有什么理解？</p>
</li>
<li><p>你为HugeGraph的Vermeer框架增加了“优先级调度”和“依赖检测” 。请具体说明你是如何实现的？这在图计算场景下有什么具体的应用价值？</p>
</li>
<li><p>作为开源之夏项目，你是如何与社区进行协作和讨论的 ？你的代码贡献（Pull Request）是如何被接受和合并的？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（对实习的理解）</strong> 这段在百度的开源之夏经历 ，是我第一次深度参与大型开源社区（HugeGraph）的宝贵经验。我的核心理解是，<strong>开源协作的本质是规范、沟通与信任</strong>。</p>
<ol>
<li><p><strong>技术上</strong>，我深入了解了Vermeer这个高性能图计算框架的内部架构，特别是其调度器模块。</p>
</li>
<li><p><strong>协作上</strong>，我学会了如何通过GitHub Issues、Pull Requests (PR) 和社区讨论 ，清晰地阐述我的设计思路（如优先级调度，以及小任务并行），理解并遵循社区的编码规范和CI&#x2F;CD流程，并根据社区导师和Reviewer的反馈迭代我的代码。这段经历锻炼了我的代码规范性和远程协作能力。</p>
</li>
</ol>
</li>
<li><p><strong>（依赖检测的实现与价值）</strong></p>
<ol>
<li><p>先写一个稿子，表述中心句</p>
</li>
<li><p>这是一个基于<strong>分布式图计算引擎 HugeGraph Vermeer</strong> 的<strong>任务调度子系统优化</strong>项目。</p>
</li>
<li><p>核心目标是解决现有调度系统的两大痛点：<strong>缺乏流程控制</strong>和<strong>资源分配僵化</strong>。</p>
<ul>
<li><p><strong>保证复杂流程的正确性：</strong> 在复杂的图分析流程中（如 加载-&gt;计算），缺乏<strong>依赖检测</strong>会导致后继任务在数据未就绪时提前执行，造成计算错误或系统崩溃。</p>
</li>
<li><p><strong>提升调度效率与健壮性：</strong> 在多租户或混合负载（实时查询 vs. 离线分析）环境中，缺乏<strong>优先级调度</strong>，无法确保关键业务优先获得资源，影响系统的稳定性和响应速度。</p>
</li>
</ul>
</li>
<li><p>我通过<strong>重构调度子系统</strong>实现了“优先级调度”和“依赖检测”：</p>
<ul>
<li><strong>增强任务结构体 (<strong><strong><code>TaskInfo</code></strong></strong>)：</strong></li>
</ul>
<ol>
<li><p>新增 <code>Priority</code> 字段：定义任务执行的优先级。</p>
</li>
<li><p>新增 <code>Preorders</code> 字段：定义任务必须等待完成的前驱依赖任务列表。</p>
</li>
</ol>
<ul>
<li><strong>重构调度核心 (<strong><strong><code>SchedulerAlgorithmManager</code></strong></strong>) 并新增算法：</strong></li>
</ul>
<ol>
<li><p><strong>实现</strong> <strong><code>Depends</code></strong> <strong>算法（依赖检测）：</strong> 严格检查任务的 <code>Preorders</code> 字段。仅当前驱任务<strong>全部完成</strong>后，才允许任务进入执行队列，确保了复杂图计算流程的<strong>正确性</strong>。</p>
</li>
<li><p><strong>实现</strong> <strong><code>Priority</code></strong> <strong>和</strong> <strong><code>PriorityElder</code></strong> <strong>算法（优先级调度）：</strong> 确保高优先级的关键业务（如实时查询）能优先获得计算资源，极大地提升了系统的<strong>效率</strong>和<strong>资源分配灵活性</strong>。</p>
</li>
</ol>
</li>
<li><p>旧版</p>
</li>
</ol>
</li>
</ol>
<ul>
<li><p><strong>实现方式</strong>：我通过<strong>重构调度子系统</strong>实现了“优先级调度”和“依赖检测”：在任务结构体 <code>TaskInfo</code> 中，新增了 <code>Priority</code>（用于定义优先级）和 <code>Preorders</code>（用于定义前驱依赖）字段。使用多个调度管理器，其中调度核心 <code>SchedulerAlgorithmManager</code> 新增了 <code>Depends</code> 算法来严格确保任务按 <code>Preorders</code> 字段定义的顺序执行，防止前驱任务未完成时后继任务就开始执行，保证了复杂图计算流程的<strong>正确性</strong>。同时，调度器实现了 <code>Priority</code> 和 <code>PriorityElder</code> 等算法，确保高优先级的关键业务（如实时查询）能优先获得计算资源，极大提升了图计算场景下系统调度的<strong>健壮性、效率和资源分配的灵活性</strong>，特别是在复杂的分析流水线和多租户环境中价值巨大。</p>
</li>
<li><p><strong>应用价值</strong>：在复杂的图计算中，依赖检测 是必须的。例如，一个典型的图分析流程可能是：<strong>Step 1: 图加载</strong> -&gt; <strong>Step 2: 子图裁剪</strong> -&gt; <strong>Step 3: 运行PageRank</strong>。</p>
<ul>
<li><p>如果没有依赖检测 ，PageRank任务（Step 3）很可能在图数据尚未加载（Step 1）或裁剪（Step 2）完成时就开始执行，导致计算错误或系统崩溃。</p>
</li>
<li><p>通过我增加的依赖检测功能 ，用户可以定义一个完整的、复杂的分析流水线，并确保它们<strong>按正确的顺序自动化执行</strong>，极大地提升了框架的健壮性和易用性。</p>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li><p><strong>（开源社区协作）</strong> 我参与社区 的流程非常规范：</p>
<ol>
<li><p><strong>认领任务</strong>：我首先在社区的Issue列表里找到了关于“增强调度器功能”的任务（Task），并与社区导师沟通，确认了我的兴趣和可行性。</p>
</li>
<li><p><strong>设计文档</strong>：在编码之前，我先编写了一个简短的设计文档（Design Doc），阐述了我实现优先级和依赖检测的思路，并发布到社区邮件组进行讨论 。</p>
</li>
<li><p><strong>编码与PR</strong>：获得导师的初步认可后，我Fork了项目仓库，在本地创建了Feature分支进行开发。完成后，我提交了Pull Request (PR) 到主仓库。</p>
</li>
<li><p><strong>CI与Review</strong>：PR触发了社区的CI（持续集成）流水线，包括代码风格检查、编译和单元测试。同时，社区导师和几位Commiter对我的代码进行了详细的Code Review，指出了几处潜在的并发问题和需要补充的单元测试。</p>
</li>
<li><p><strong>迭代与合并</strong>：我根据Review意见进行了修改和补充，并再次提交。目前即将合入主分支。</p>
</li>
</ol>
</li>
</ol>
<hr>
<h4 id="3-九维数智-Jiuwei-Data"><a href="#3-九维数智-Jiuwei-Data" class="headerlink" title="3. 九维数智 (Jiuwei Data)"></a>3. 九维数智 (Jiuwei Data)</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>你对九维数智的这段基础架构实习有什么理解？</p>
</li>
<li><p>（深度追问）你实现了无锁数据结构（链表、队列、栈） ，并提到“万级并发时延小于1ms” 。你是如何测试和验证无锁代码的<strong>正确性</strong>（这非常困难）和<strong>性能</strong>的？你主要运用了哪些无锁编程技术？</p>
</li>
<li><p>你开发了“面向nfs的csi插件” 并已在多家大规模超算集群稳定运行 。为什么选择NFS？相比于Ceph（你在大岩资本用过 ）或GlusterFS，NFS在超算集群场景下有什么特别的优势？</p>
</li>
<li><p>你提到优化文件系统pnfs驱动模块，提升了5%的传输速度 。5%的提升在驱动层已经非常显著了，你是如何通过“块粒度通信方式” 实现这个优化的？</p>
</li>
<li><p>你开发了gRPC-like的网络库 ，提到使用“多组件线程池和epoll方式减少处理时间多达50%” 。请详细解释一下什么是“多组件线程池”，它和单个大线程池相比优势在哪里？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（对实习的理解）</strong> 在九维数智的实习，是我在<strong>底层基础架构</strong>领域最全面、最深入的实践。我的理解是，所有上层的高性能应用（无论是超算还是大模型），都依赖于极致优化的基础组件。</p>
<ol>
<li><p>我的工作横跨了<strong>系统编程</strong>（无锁数据结构 、网络库 ）、<strong>操作系统内核</strong>（pNFS驱动优化）、<strong>容器编排</strong>（K8s CSI插件 ）和<strong>AI系统</strong>（大模型分布式框架 ）四个层面。</p>
</li>
<li><p>这段经历让我深刻体会到，基础架构的优化是“环环相扣”的。例如，我实现的低延迟无锁队列被用于我开发的高性能网络库中；而网络库和CSI插件又是支撑大模型分布式框架的基石。这让我构建了从底层并发到上层分布式AI系统的完整技术图景。</p>
</li>
</ol>
</li>
<li><p><strong>（无锁数据结构）</strong></p>
<ol>
<li><p><strong>核心技术</strong>：实现无锁的核心技术是<strong>CAS（Compare-and-Swap）原子原语。在实现无锁栈时，则通过CAS循环来原子地更新栈顶指针。我们还必须处理ABA问题</strong>，通常是通过使用“标记指针”（Tagged Pointers）或双字CAS（Double-Word CAS）来将版本号和指针捆绑在一起原子更新。</p>
</li>
<li><p><strong>正确性验证</strong>：这是最难的部分。我们采用了多重手段：</p>
<ul>
<li><p><strong>代码审查</strong>：严格审查每一行原子操作的内存序（Memory Ordering），确保在多架构（如x86的TSO和ARM的Weak Memory Model）以及多平台（如 Linux 和 Windows）上都能正确执行 。</p>
</li>
<li><p><strong>压力测试</strong>：我们编写了多线程压力测试程序，在万级并发下长时间（例如1小时）运行，对数据结构进行高强度的混合读写（Push&#x2F;Pop），并校验结果的一致性。</p>
</li>
</ul>
</li>
<li><p><strong>性能测试</strong>：我们在一个多核（例如64核）服务器上进行基准测试，同时通过核调度将我们的程序独占CPU。我们模拟了万级并发的线程，并使用高精度时钟（如<code>rdtsc</code>或<code>std::chrono</code>）来测量每次操作的P99（99th percentile）时延，最终验证了其在饱和负载下时延仍小于1ms 。</p>
</li>
</ol>
</li>
<li><p><strong>（NFS CSI 插件）</strong> 我们在超算集群场景下选择NFS，主要是出于<strong>兼容性、存量优势</strong>和<strong>性能</strong>的综合考量。</p>
<ol>
<li><p><strong>存量优势与兼容性</strong>：许多大规模超算中心已经部署了极其庞大且高度优化的NFS（特别是pNFS ）存储集群。NFS客户端内嵌于所有Linux内核，运维简单，兼容性极好。我们的CSI插件能够无缝利旧这些现有的、价值高昂的存储设施。</p>
</li>
<li><p><strong>性能优势</strong>：虽然Ceph提供了很好的扩展性和统一存储特性，但在超算场景下（常见大文件顺序读写），高性能的pNFS（并行NFS）表现非常出色。pNFS允许客户端绕过元数据服务器，直接与多个存储节点（Data Servers）并行通信，吞吐量极高。</p>
</li>
<li><p><strong>我们的插件价值</strong>：原生的NFS在K8s中缺乏精细化管理。我们的CSI插件 提供了关键的企业级功能，如<strong>基于用户的授权认证</strong>（这在多租户超算上至关重要）、<strong>卷的扩容</strong>、<strong>配额管理</strong>和<strong>快照管理</strong> 。这使得K8s能以云原生的方式，安全、高效地使用超算中心已有的NFS存储。</p>
</li>
</ol>
</li>
<li><p><strong>（pNFS 驱动优化）</strong> pNFS的核心思想是“元数据”和“数据”分离。我优化的点在于客户端和数据服务器（Data Server）的通信协议上。</p>
<ol>
<li><p><strong>原始方式</strong>：我们的存储服务是基于GPFS修改的，但是GPFS默认只支持以文件粒度的pNFS。</p>
</li>
<li><p><strong>块粒度通信（我的优化）</strong>：我的优化是将GPFS以文件粒度的调用变为以块粒度的调用，这样能以对存储节点更友好的方式来发送数据。</p>
</li>
<li><p><strong>结果</strong>：例如，原先使用文件粒度，需要以文件形式与存储节点通信，由存储节点负责解析文件，将其变为向存储块写入。而使用块的方式，只需要完全和块设备通信即可。这种方式减少了降低了存储节点上的CPU开销，并更充分地利用了网络带宽。在特定的大文件读写负载下，最终测试显示传输速度提升了约5% 。</p>
</li>
</ol>
</li>
<li><p><strong>（多组件线程池）</strong> “多组件线程池” 是对传统Reactor模型（如gRPC所用）的一种精细化实现，其核心思想是<strong>“职责分离”</strong>。</p>
<ol>
<li><p><strong>传统线程池的问题</strong>：如果使用一个大的线程池（Worker Pool）来处理所有事情（接受连接、IO读写、业务逻辑），那么一个耗时的业务逻辑计算（CPU密集型）可能会长时间占据一个线程，导致该线程无法及时处理其他套接字（Socket）的IO事件（IO密集型），造成“队头阻塞”（Head-of-Line Blocking），延迟飙升。</p>
</li>
<li><p><strong>我的“多组件”实现</strong>：</p>
<ul>
<li><p><strong>IO组件（Reactor）</strong>：我们有1-2个专门的线程（或小线程池），它们只负责<code>epoll</code> 循环。当<code>epoll</code> 监测到IO就绪事件（如数据可读）时，这个线程只负责从Socket读取数据，然后将数据包（Task）封装好，<strong>抛入</strong>到“Worker组件”的队列中。IO线程本身<strong>不执行任何业务逻辑</strong>。</p>
</li>
<li><p><strong>Worker组件（Proactor）</strong>：这是一个独立的、更大规模的线程池 ，它从队列中获取任务包，执行真正的业务逻辑（如RPC的解码、计算、编码）。</p>
</li>
</ul>
</li>
<li><p><strong>优势</strong>：通过<code>epoll</code> 和这种“IO线程”与“Worker线程”的分离，保证了IO线程永远不会被阻塞，能够以极高的效率响应<code>epoll</code> 事件。Worker线程池则可以根据CPU核心数配置，专心处理计算。这种架构避免了IO与计算的相互干扰，使得网络库的整体处理时延在高并发下减少了多达50% 。</p>
</li>
</ol>
</li>
<li><p>大模型框架<br>  本项目是基于现有 <strong>vLLM&#x2F;LM Cache</strong> 技术栈深度定制化的高性能 AI 推理基础设施核心组件。<br>  在 LLM 推理场景中，<strong>Prefix Caching (前缀缓存)</strong> 是提高效率的关键。然而，传统的 KV Cache 机制和通用文件系统面临挑战：</p>
<ol start="3">
<li><p>挑战一：小文件 I&#x2F;O 效率低下 (存储瓶颈)</p>
<ul>
<li><strong>瓶颈:</strong> KV Cache 单元本质上是大量分散的、尺寸较小的文件或数据块。通用文件系统对这种<strong>高并发、小尺寸</strong>的 I&#x2F;O 模式处理效率不高，导致缓存的存取延迟较高。</li>
</ul>
</li>
<li><p>挑战二：高频缓存的查找延迟 (访问瓶颈)</p>
<ul>
<li><strong>瓶颈:</strong> 随着缓存数量的增加，查找并定位所需 KV Cache 文件需要经过多次索引或目录遍历，高频（热点）前缀缓存的访问延迟依然是瓶颈。</li>
</ul>
</li>
<li><p>挑战三：数据完整性与可靠性</p>
<ul>
<li><strong>瓶颈:</strong> 高速 I&#x2F;O 操作中，KV Cache 数据的正确性、完整性校验至关重要，需要确保读取的数据是精确且完整的。</li>
</ul>
</li>
</ol>
</li>
</ol>
<p>    解决方案：</p>
<ol start="7">
<li><p>NVFS 专为快速处理小文件而设计，能显著降低单个 KV Cache 文件的读写延迟，提高存储系统的<strong>I&#x2F;O 吞吐量</strong>。</p>
</li>
<li><p>引入<strong>前缀树</strong>数据结构来组织和索引 KV Cache 文件。将高频、常用的 KV Cache（对应短前缀）放置在缓存树的顶部。显著<strong>减少查找路径</strong>，实现对热点缓存的<strong>极速定位和访问</strong>，有效降低高频请求的整体 IO Latency。</p>
</li>
<li><p>为每个 KV Cache 分块设计详细的元信息，包括<strong>哈希值（Checksum）和精确长度&#x2F;尺寸</strong>等。通过哈希值校验，确保数据读写过程中的<strong>完整性</strong>；通过长度校验，避免加载不匹配或损坏的缓存，极大地提升了推理服务的<strong>可靠性</strong>。</p>
</li>
</ol>
<hr>
<h4 id="4-湖杉科技-Hushan-Tech"><a href="#4-湖杉科技-Hushan-Tech" class="headerlink" title="4. 湖杉科技 (Hushan Tech)"></a>4. 湖杉科技 (Hushan Tech)</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>你对湖杉科技的这段后端开发总负责和运维的实习有什么理解？</p>
</li>
<li><p>你的职位是“后端开发总负责，运维” ，这是一个责任非常重大的角色。你能否具体描述一下你的职责？你如何协调开发任务和运维压力的？</p>
</li>
<li><p>你提到“利用LoRa优化大模型，用户满意度提升20%” 。LoRa（Low-Rank Adaptation）是一种微调技术，它具体是如何帮助提升“用户满意度”的？是实现了个性化，还是降低了推理延迟？</p>
</li>
<li><p>你“自编写的调度系统” 是如何维持AI模型和GPU服务器负载均衡的 ？它的调度策略（Policy）是怎样的？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（对实习的理解）</strong> 在湖杉科技的经历，对我而言是一次从<strong>纯技术执行者</strong>向<strong>技术负责人</strong>转变的锻炼。作为“总负责” ，我不仅要写代码，更要对系统的<strong>整体效率</strong>和<strong>稳定性</strong>负责。</p>
<ol>
<li><p><strong>效率方面</strong>：我需要思考如何最大化利用昂贵的GPU资源。因此我基于K8s自研了调度系统 ，实现了GPU服务器的负载均衡 ，最终将资源利用率提升了10% 。</p>
</li>
<li><p><strong>业务方面</strong>：技术最终要服务于用户。我通过调研和实践LoRa技术来优化大模型，实现了更贴近用户需求的模型服务，带来了20%的用户满意度提升 。</p>
</li>
<li><p><strong>运维方面</strong>：我作为运维 ，需要确保多台16卡服务器的稳定。这段经历让我深刻理解了DevOps文化，即开发必须从第一天起就考虑系统的可观测性、可部署性和稳定性。</p>
</li>
</ol>
</li>
<li><p><strong>（总负责的职责）</strong> 作为“后端开发总负责” ，我的职责是端到端的：</p>
<ol>
<li><p><strong>架构设计</strong>：我负责后端AI推理服务的整体技术选型和架构设计。例如，我决定采用K8s 作为资源管理的底座，并主导设计了上层的GPU负载均衡调度系统 。</p>
</li>
<li><p><strong>开发执行</strong>：我主导并编写了后端的核心代码，包括调度系统、以及LoRa模型优化 相关的服务。</p>
</li>
<li><p><strong>运维保障（Ops）</strong>：我同时承担运维角色，负责所有服务的部署、监控（Monitoring）、告警（Alerting）和故障排查。我需要确保多台16卡服务器上的AI模型 始终处于稳定平衡的负载 。</p>
</li>
<li><p><strong>协调</strong>：当开发新功能（Dev）和处理线上故障（Ops）冲突时，我的优先级始终是<strong>保障线上稳定（Ops）</strong>。我会利用自动化脚本和K8s的自愈能力来减少运维压力，然后利用“空闲”时间窗口来推进开发任务。</p>
</li>
</ol>
</li>
<li><p><strong>（LoRa 提升用户满意度）</strong> LoRa是一种参数高效性微调（PEFT）技术。它提升用户满意度20% 的关键在于<strong>低成本地实现了“模型个性化”</strong>。</p>
<ol>
<li><p><strong>问题</strong>：通用的基础大模型（Base Model）在特定领域或特定用户的口吻上表现不佳。而为每个用户微调一个完整的模型，成本（显存和存储）高到无法接受。</p>
</li>
<li><p><strong>LoRa的解决方案</strong>：</p>
<ul>
<li><p>我们只维护一个基础大模型。</p>
</li>
<li><p>对于不同的用户或不同的业务场景（例如“模特修改” vs “珠宝美化”），我们训练<strong>不同</strong>的、但<strong>非常小</strong>（只有几MB到几十MB）的LoRa适配器（Adapter）。</p>
</li>
<li><p>当用户的请求到来时，我们的后端服务会<strong>动态加载</strong>（Hot-Swap）基础模型和<strong>该用户专属的LoRa适配器</strong>。</p>
</li>
</ul>
</li>
<li><p><strong>结果</strong>：用户感知到的是一个“懂他”的、经过定制的模型，回复质量更高，因此满意度显著提升 。而对我们后端来说，我们只需要极小的额外存储和显存开销（LoRa适配器的开销），就能提供大规模的个性化服务。</p>
</li>
</ol>
</li>
<li><p><strong>（自研调度系统策略）</strong> K8s 原生的调度器（kube-scheduler）对GPU的感知很粗糙，它只知道“节点上有几张卡”，但不知道“每张卡的负载”。我们的<strong>自编写的调度系统</strong>是一个运行在K8s之上的<strong>自定义调度器（Custom Scheduler）</strong>。</p>
<ol>
<li><p><strong>策略核心</strong>：我们的核心策略是<strong>“基于实时负载的精细化装箱（Bin Packing）”</strong>。</p>
</li>
<li><p><strong>实现步骤</strong>：</p>
<ul>
<li><p><strong>监控</strong>：我们通过Prometheus Exporter，实时收集集群中每台16卡服务器 上<strong>每一张GPU卡</strong>的<strong>显存使用率</strong>和<strong>SM（计算单元）利用率</strong>。</p>
</li>
<li><p><strong>调度</strong>：当一个AI模型的推理请求（Pod）需要被调度时，K8s默认调度器会“挂起”这个Pod，交由我们的自定义调度器处理。</p>
</li>
<li><p><strong>决策</strong>：我们的调度器会遍历所有节点的GPU实时负载数据，并根据一个<strong>综合评分函数</strong>（综合考虑了“剩余显存”和“SM空闲率”）来选择一个<strong>当前最空闲</strong>且<strong>资源足够</strong>的GPU卡，然后将Pod调度到该卡所在的节点上。</p>
</li>
</ul>
</li>
<li><p><strong>结果</strong>：这个策略避免了K8s默认调度可能导致的“某些卡被打满，另一些卡却空闲”的负载不均情况，确保了所有GPU服务器都能维持稳定平衡的负载，从而将整体资源利用率提升了10% 。</p>
</li>
</ol>
</li>
</ol>
<hr>
<h3 id="二、-项目经历-Project-Experience"><a href="#二、-项目经历-Project-Experience" class="headerlink" title="二、 项目经历 (Project Experience)"></a>二、 项目经历 (Project Experience)</h3><h4 id="1-P-D-动态拆分合并的推理引擎系统"><a href="#1-P-D-动态拆分合并的推理引擎系统" class="headerlink" title="1. P-D 动态拆分合并的推理引擎系统"></a>1. P-D 动态拆分合并的推理引擎系统</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>你这个项目是基于vllm做的优化。vllm已经通过PagedAttention解决了KV Cache的显存瓶颈，你认为vllm在调度层面还有什么不足？</p>
</li>
<li><p>你提到提出了“Prefill和Decode性能建模模块” ，并能预测任意batch的执行时间（准确度&gt;95%） 。请问你是如何建立这个模型的？它考虑了哪些关键参数？</p>
</li>
<li><p>请解释一下你的“SLO感知和级联超时检测技术” 是如何工作的？它如何帮助goodput平均提升超过10% ？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（vllm的不足）</strong> vllm的PagedAttention确实极大地提高了显存利用率和吞吐量。但它的调度策略（如默认的FIFO）相对简单，它主要解决了<strong>“空间”</strong>（显存）上的问题，但在<strong>“时间”</strong>（调度）上还有不足：</p>
<ol>
<li><p><strong>Prefill与Decode阶段的冲突</strong>：vllm会把Prefill（处理提示词）和Decode（生成Token）的请求合并到同一个batch中。Prefill是一个<strong>计算密集型</strong>操作，而Decode是一个<strong>访存密集型</strong>操作。当一个batch中混入了长的Prefill任务时，会导致整个batch的执行时间变长，使得batch中其他的Decode任务（它们本该很快完成）被迫等待，导致<strong>Token间延迟（Inter-Token Latency）</strong>飙高。</p>
</li>
<li><p><strong>缺乏SLO感知</strong>：vllm的调度器不区分“即将超时”的请求和“刚来”的请求，这在生产系统中是不够的。</p>
</li>
</ol>
</li>
<li><p><strong>（性能建模）</strong> 我们的“Prefill和Decode性能建模模块” 是我们实现SLO感知调度的基础。</p>
<ol>
<li><p><strong>建模思路</strong>：我们没有使用复杂的机器学习模型，而是采用了<strong>基于分析的白盒建模</strong>。我们将一个batch的执行时间拆分为Prefill阶段耗时和Decode阶段耗时 。</p>
</li>
<li><p><strong>关键参数</strong>：</p>
<ul>
<li><p><strong>Prefill耗时</strong>：主要与该batch中所有Prefill请求的<strong>总提示词（Prompt）Token数</strong>和<strong>计算瓶颈</strong>（如GEMM操作）相关。我们通过离线Profiling（分析）得到了一个“单位Token的Prefill计算时间”系数。</p>
</li>
<li><p><strong>Decode耗时</strong>：主要与该batch中<strong>总请求数（Batch Size）和访存瓶颈</strong>（KV Cache的读写）相关。我们也通过Profiling得到了一个“单位Token的Decode访存时间”系数。</p>
</li>
</ul>
</li>
<li><p><strong>模型</strong>：最终的模型是一个<strong>分段线性函数</strong>，输入是“任意batch”的Prefill Token总数和Decode请求总数，输出就是预测的执行时间 。通过在真实负载上校准参数，我们实现了高于95%的预测准确度 。</p>
</li>
</ol>
</li>
<li><p><strong>（SLO与级联超时检测）</strong> “Goodput” 指的是<strong>在SLO（服务等级目标，如2秒内必须返回）之内</strong>成功完成的请求。</p>
<ol>
<li><p><strong>SLO感知</strong> ：我们的调度器在选择下一个batch时，会优先选择那些“剩余SLO时间”最短、即将超时的请求，确保它们被尽快处理。</p>
</li>
<li><p><strong>级联超时检测</strong> ：这是更关键的优化。</p>
<ul>
<li><p><strong>问题</strong>：如果调度器为了凑一个大batch，把一个“即将超时”的请求A和一个“超长”的Prefill请求B（它自己不超时）放进同一个batch，会发生什么？</p>
</li>
<li><p><strong>预测</strong>：我们的调度器会调用A2中提到的<strong>性能模型</strong> ，<strong>预测</strong>这个（A+B）batch的执行时间。</p>
</li>
<li><p><strong>检测</strong>：调度器发现，预测的执行时间 &gt; 请求A的剩余SLO。</p>
</li>
<li><p><strong>决策</strong>：调度器会判定这是一个“坏”batch，它会导致请求A超时（即产生badput）。因此，调度器会<strong>拒绝</strong>将A和B合并，而是优先执行A。</p>
</li>
</ul>
</li>
<li><p><strong>结果</strong>：通过这种“预测性”的<strong>级联超时检测</strong> ，我们避免了调度器为了提高吞吐量（Throughput）而“牺牲”即将超时的请求。这使得系统更智能地在“吞吐量”和“延迟”之间做权衡，确保了绝大多数请求都能在SLO内完成，因此“goodput”平均提升了超过10% 。</p>
</li>
</ol>
</li>
</ol>
<hr>
<h4 id="2-适用于生产系统的动态软件更新框架"><a href="#2-适用于生产系统的动态软件更新框架" class="headerlink" title="2. 适用于生产系统的动态软件更新框架"></a>2. 适用于生产系统的动态软件更新框架</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>你提到“使用了批量复制的方式达到较低的CPU开销，并提高了21.74倍的更新性能” 。请问你复制的是什么？相比于传统的更新方式，这个“批量复制” 的核心优势在哪？</p>
</li>
<li><p>这个框架是如何做到“基本无感”更新的？“只在空闲时间以增量方式进行更新” 的技术细节是怎样的？</p>
</li>
<li><p>这个框架减少了生产损失约75%。这是如何衡量的？它解决了生产中的什么痛点？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（批量复制）</strong></p>
<ol>
<li><p><strong>复制内容</strong>：在动态软件更新时，最耗时的步骤是迁移<strong>旧版本程序的状态（State）到新版本，这主要包括全局变量</strong>和<strong>堆内存（Heap）</strong>上的数据。</p>
</li>
<li><p><strong>传统方式</strong>：传统的动态更新（如基于<code>dlopen</code>或某些语言的热更）可能需要逐个对象（Object-by-Object）地序列化、传输和反序列化，这个过程涉及大量的CPU开销 和类型检查。</p>
</li>
<li><p><strong>我的优化（批量复制）</strong>：我的优化在于，框架能够识别出大块的、连续的、且在新旧版本间布局兼容的内存区域。然后，我们<strong>不关心</strong>这些内存里具体是什么对象，而是直接调用<code>memcpy</code>，以<strong>批量（Batch）</strong>的方式将整个内存页（Page）或内存块（Chunk）从旧进程空间复制到新进程空间。这种方式极大减少了CPU的计算和遍历开销 ，使得状态迁移的纯粹性能（不考虑暂停时间）提升了21.74倍 。</p>
</li>
</ol>
</li>
<li><p><strong>（无感更新与增量更新）</strong> “基本无感” 指的是更新过程中的<strong>“停服时间”（Stop-the-World Pause）极短</strong>。这是通过“增量更新” 实现的：</p>
<ol>
<li><p><strong>空闲识别</strong>：这得益于PLC独特的执行特点。PLC周期性执行，每个周期均会有空闲时间。当检测到系统处于低负载（即“空闲时间”） 时，更新流程启动。</p>
</li>
<li><p><strong>后台增量复制</strong>：系统<strong>不会暂停</strong>，而是在后台开始复制（使用A1中提到的批量复制 ）程序状态。</p>
</li>
<li><p><strong>写时复制（Copy-on-Write）</strong>：在后台复制期间，生产系统 仍在运行。如果系统修改了某块<strong>尚未被复制</strong>的内存，一切正常。如果它试图修改某块<strong>已经被复制</strong>的内存，我们会触发一个（类似于Copy-on-Write的）机制，标记这块内存为“脏”（Dirty），并暂存这个修改。</p>
</li>
<li><p><strong>最终切换（Cutover）</strong>：当绝大多数（例如99%）的状态都在后台增量 复制完毕后，框架会发起一个极短的“停服”：(a) 暂停系统；(b) 复制剩余的“脏”数据；(c) 将流量和控制权切换到新版本。</p>
</li>
<li><p><strong>结果</strong>：由于绝大部分工作都在后台的“空闲时间” 完成了，最终“停服”的窗口期非常短（毫秒级），从而实现了“基本无感” 更新。</p>
</li>
</ol>
</li>
<li><p><strong>（减少生产损失）</strong></p>
<ol>
<li><p><strong>痛点</strong>：在传统的生产系统中（例如7x24小时运行的制造执行系统 MES），如果发现一个Bug或需要上线一个优化，必须停机维护。停机1小时，意味着生产线（例如芯片制造或装配线）也必须停工1小时，这会带来巨大的生产损失 。</p>
</li>
<li><p><strong>解决方案</strong>：我们的动态软件更新框架 允许在<strong>不停止生产</strong>的情况下，“热修复”（Hotfix）Bug或“热更新”（Hot-Update）优化 。</p>
</li>
<li><p><strong>衡量方式</strong>：损失减少75%是这样估算的：假设以前某个关键系统平均每月需要停机维护4小时才能完成更新。使用了我们的框架后，90%的更新（如小的Bug修复）都可以在“无感” 状态下完成，完全不停机；剩下10%的大版本更新，停机时间也因为我们的优化 而大幅缩短。综合计算，原先的停机时间被减少了约75%，这直接等同于减少了75%的生产损失 。</p>
</li>
</ol>
</li>
</ol>
<hr>
<h4 id="3-多-Agent-调度框架"><a href="#3-多-Agent-调度框架" class="headerlink" title="3. 多 Agent 调度框架"></a>3. 多 Agent 调度框架</h4><p><strong>面试官提问：</strong></p>
<ol>
<li><p>这个项目是基于Langchain的多Agent协作框架 。你提到的“调度器” 在这个框架中扮演什么角色？是Agent之间的任务分派，还是“模型生成的代码” 的执行调度？</p>
</li>
<li><p>你提到这个框架适用于“生物等领域” 。为什么生物领域特别需要这种多Agent协作？能否举一个具体的协作流程例子？</p>
</li>
</ol>
<p><strong>面试者回答：</strong></p>
<ol>
<li><p><strong>（调度器角色）</strong></p>
</li>
<li><p>在这个框架中，调度器（特指 AutoDNA 的 <strong>C++ Scheduler</strong>）扮演的是<strong>“硬件协调中心”</strong>和<strong>“执行管理”</strong>的角色，它与上层的多 Agent 系统（AutoDNA-python）协作。</p>
<ol>
<li><p><strong>任务分派（Agent间&#x2F;协议生成）</strong>：</p>
<ul>
<li><p><strong>AutoDNA-python</strong>（多 Agent 系统）负责高级任务分派。例如，“规划 Agent”将任务分配给“协议生成 Agent”。</p>
</li>
<li><p><strong>协议生成 Agent</strong> 基于用户的需求，生成<strong>可执行的实验协议代码</strong>。</p>
</li>
</ul>
</li>
<li><p><strong>代码执行调度（Agent与硬件间）</strong>：这是 <strong>C++ Scheduler</strong> 的核心功能。它负责将模型生成的代码（实验协议&#x2F;步骤）转化为实际的物理操作。</p>
<ul>
<li><p><strong>调度器</strong> 接收到<strong>“协议生成 Agent”</strong>输出的实验协议（即硬件操作指令）。</p>
</li>
<li><p>它会<strong>高效地将协议部署执行</strong>，具体步骤包括：</p>
</li>
<li><p><strong>硬件协调</strong>：管理实验室自动化硬件（如移液站、热循环仪）的资源和工作流。</p>
</li>
<li><p><strong>指令翻译</strong>：将高层协议转化为对底层 <strong>Firmware</strong> (C# 和 PLC 程序) 的低级控制信号。</p>
</li>
<li><p><strong>安全执行与监控</strong>：驱动硬件执行操作，并实时捕获硬件状态和实验结果。</p>
</li>
<li><p>最后，调度器将实验结果反馈给 <strong>AutoDNA-python</strong> 中的后续 Agent（如“分析 Agent”）。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>（生物领域应用实例）</strong></p>
<ol>
<li><p><strong>为什么是生物领域</strong>：生物领域（如分子生物学、药物筛选）的研究流程具有<strong>高精度要求、高重复性需求和对“工具&#x2F;计算”的强依赖性</strong>。</p>
<ul>
<li><p>研究员需要 <strong>LLM (AutoDNA-python)</strong> 来阅读文献、设计实验（知识与推理），但也<strong>必须</strong>运行精确的物理实验（需要<strong>自动化硬件</strong>）。</p>
</li>
<li><p>多Agent框架结合硬件调度，完美契合了<strong>“AI生成实验协议”</strong>与<strong>“自动化硬件执行”</strong>的端到端需求。</p>
</li>
</ul>
</li>
<li><p><strong>协作流程例子</strong>：</p>
<ul>
<li><p><strong>用户</strong> 提出需求：“帮我自动化执行一个 CRISPR 基因编辑的sgRNA筛选实验。”</p>
</li>
<li><p><strong>调度器 (Agent)</strong> 激活 <strong>“规划 Agent”</strong>。</p>
</li>
<li><p><strong>规划 Agent</strong>（基于 LLM）输出计划：(1) sgRNA序列设计；(2) 寡核苷酸合成与退火；(3) 慢病毒包装；(4) 细胞转染与筛选。</p>
</li>
<li><p><strong>调度器 (Agent)</strong> 批准计划，并激活 <strong>“协议生成 Agent”</strong>。</p>
</li>
<li><p><strong>协议生成 Agent</strong> 针对步骤(2)和(3)生成了<strong>自动化移液和温控的操作协议（代码）</strong>。</p>
</li>
<li><p><strong>C++ Scheduler</strong> 接收协议，并将其交给 <strong>Firmware</strong> <strong>在自动化工作站中执行</strong>。</p>
</li>
<li><p><strong>C++ Scheduler</strong> 协调机械臂进行精确的试剂分配、孵育和洗涤操作，并返回执行结果。</p>
</li>
<li><p><strong>调度器 (Agent)</strong> 激活 <strong>“数据分析 Agent”</strong>，处理筛选得到的原始数据（如高通量测序数据）。</p>
</li>
<li><p>最后，<strong>调度器 (Agent)</strong> 将所有结果汇总，交给 <strong>“总结 Agent”</strong>，生成一份完整的筛选报告给用户。</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="三、顽岩面试"><a href="#三、顽岩面试" class="headerlink" title="三、顽岩面试"></a>三、顽岩面试</h3><ol>
<li><h4 id="K8S-里面我们去创建一个-Deployment-的这样的一个资源对象…-这个过程中会涉及到哪些-K8S-的组件？"><a href="#K8S-里面我们去创建一个-Deployment-的这样的一个资源对象…-这个过程中会涉及到哪些-K8S-的组件？" class="headerlink" title="K8S 里面我们去创建一个 Deployment 的这样的一个资源对象… 这个过程中会涉及到哪些 K8S 的组件？"></a>K8S 里面我们去创建一个 <code>Deployment</code> 的这样的一个资源对象… 这个过程中会涉及到哪些 K8S 的组件？</h4></li>
</ol>
<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>步骤</td>
<td>组件</td>
<td>职责&#x2F;流程描述</td>
</tr>
<tr>
<td>1. 客户端交互</td>
<td>kubectl &#x2F; APIServer</td>
<td>用户通过 kubectl apply 命令，将 Deployment YAML&#x2F;JSON 文件发送给 kube-apiserver。APIServer 对请求进行认证、授权和校验，并将 Deployment 对象的数据写入 etcd 存储。</td>
</tr>
<tr>
<td>2. Deployment 监听</td>
<td>Deployment Controller</td>
<td>kube-controller-manager 中的 Deployment Controller 监听 APIServer 的变化。它从 etcd 中发现了一个新的 Deployment 对象。</td>
</tr>
<tr>
<td>3. ReplicaSet 创建</td>
<td>Deployment Controller</td>
<td>根据 Deployment 中定义的副本数（replicas），Deployment Controller 会创建或更新一个对应的 ReplicaSet 对象，并将其写入 etcd。</td>
</tr>
<tr>
<td>4. Pod 调度</td>
<td>ReplicaSet Controller &#x2F; Scheduler</td>
<td>接着，ReplicaSet Controller 发现新的 ReplicaSet，并根据期望的副本数创建对应的 Pod 对象。这些 Pod 刚创建时处于 Pending 状态。</td>
</tr>
<tr>
<td></td>
<td></td>
<td>kube-scheduler 监听处于 Pending 状态的 Pod。它根据预选（Predicates）和优选（Priorities）算法，选择一个最合适的 Node，并将该 Node 名称写入到 Pod 对象的 .spec.nodeName 字段中（此操作称为绑定）。</td>
</tr>
<tr>
<td>5. Pod 运行</td>
<td>Kubelet</td>
<td>每个 Node 上的 kubelet 持续监听 APIServer 中所有绑定到自己 Node 上的 Pod。一旦发现该 Pod 被调度到自己身上，Kubelet 就会调用 CRI (Container Runtime Interface) 接口，让 容器运行时（如 Docker, containerd）拉取镜像并真正启动容器。最后，Kubelet 会周期性地更新 Pod 的状态（如从 Pending 变为 Running）到 APIServer。</td>
</tr>
</tbody></table>
<ol start="2">
<li><h4 id="你能给我讲一下这个-FUSE-的基本的一个原理，或者说它实现了一种怎么样的功能？"><a href="#你能给我讲一下这个-FUSE-的基本的一个原理，或者说它实现了一种怎么样的功能？" class="headerlink" title="你能给我讲一下这个 FUSE 的基本的一个原理，或者说它实现了一种怎么样的功能？"></a>你能给我讲一下这个 FUSE 的基本的一个原理，或者说它实现了一种怎么样的功能？</h4></li>
</ol>
<p><strong>FUSE 的通用机制和优势：</strong></p>
<blockquote>
<p>FUSE (Filesystem in Userspace) 的核心功能是<strong>允许非特权用户在用户态（Userspace）创建和运行新的文件系统</strong>，而无需修改 Linux 内核代码。</p>
</blockquote>
<p><strong>原理阐述：</strong></p>
<ol>
<li><p><strong>分层架构：</strong> FUSE 机制由两部分组成：</p>
<ol>
<li><p><strong>内核模块（Kernel Module）：</strong> 负责拦截所有针对 FUSE 挂载点（Mount Point）的文件系统调用（如 <code>open()</code>, <code>read()</code>, <code>write()</code>）。</p>
</li>
<li><p><strong>用户态守护进程（FUSE Server）：</strong> 这是一个运行在用户态的程序，实现了真正的文件系统逻辑（例如，如何处理 <code>read</code> 请求，是去读本地磁盘、网络存储还是内存）。</p>
</li>
</ol>
</li>
<li><p><strong>工作流程：</strong></p>
<ol>
<li><p>当用户程序发起一个系统调用（如 <code>read(fd)</code>）到 FUSE 挂载的文件时，这个请求首先被 <strong>FUSE 内核模块</strong>捕获。</p>
</li>
<li><p>内核模块不会自己处理这个请求，而是将请求数据和参数<strong>转发</strong>（通过一个特殊设备文件）给运行在用户态的 <strong>FUSE Server</strong> 进程。</p>
</li>
<li><p>FUSE Server 执行其定制的逻辑（例如，通过网络获取数据）。</p>
</li>
<li><p>处理完成后，FUSE Server 将结果返回给 FUSE 内核模块，内核模块再返回给最初发起请求的用户程序。</p>
</li>
</ol>
</li>
<li><p><strong>优势：</strong></p>
<ol>
<li><p><strong>开发便捷性：</strong> 文件系统逻辑在用户态，崩溃不会影响内核，开发和调试更简单。</p>
</li>
<li><p><strong>灵活性：</strong> 可以在不修改内核的情况下，实现任何定制化的文件系统（例如，云存储文件系统、SSH 文件系统等）。</p>
</li>
</ol>
</li>
<li><h4 id="你用-C-重写互信息计算，提升-50-的具体优化点是什么？你是如何定位到-Python-版本的性能瓶颈的？"><a href="#你用-C-重写互信息计算，提升-50-的具体优化点是什么？你是如何定位到-Python-版本的性能瓶颈的？" class="headerlink" title="你用 C++ 重写互信息计算，提升 50% 的具体优化点是什么？你是如何定位到 Python 版本的性能瓶颈的？"></a>你用 C++ 重写互信息计算，提升 50% 的具体优化点是什么？你是如何定位到 Python 版本的性能瓶颈的？</h4></li>
</ol>
<p><strong>正确回答应体现出技术分析能力和量化手段：</strong></p>
<blockquote>
<p><strong>回答结构：</strong> 瓶颈分析 $\to$ 优化手段 $\to$ 结果。</p>
</blockquote>
<ol>
<li><p><strong>瓶颈分析与定位：</strong></p>
<ol>
<li><p><strong>定位：</strong> 我首先使用 <strong>Profiler 工具</strong>（例如 Linux 的 <code>perf</code> 或 Valgrind 的 <code>callgrind</code>）对原始 Python&#x2F;C 实现进行分析。</p>
</li>
<li><p><strong>瓶颈确认：</strong> 发现性能瓶颈主要集中在两个方面：<strong>一是频繁的内存操作和复制</strong>（尤其是处理大规模稀疏矩阵时），<strong>二是计算密集型循环缺乏向量化指令</strong>。原始 C 代码可能只是朴素实现，没有针对现代 CPU 架构进行优化。</p>
</li>
</ol>
</li>
<li><p><strong>C++ 优化手段：</strong></p>
<ol>
<li><p><strong>A. SIMD 向量化：</strong> 互信息计算涉及到大量的点积和累加运算。我使用 <strong>Intel Intrinsics</strong> 或 <strong>OpenMP SIMD</strong> 来手动或自动引导编译器使用 CPU 的 <strong>AVX&#x2F;SSE 指令集</strong>，实现数据的并行计算，这是主要的性能来源之一。</p>
</li>
<li><p><strong>B. 内存局部性优化：</strong> 确保稀疏矩阵或计算数组在内存中的<strong>连续性</strong>（Cache Line Friendly），减少 CPU 缓存不命中（Cache Miss）的概率，提高数据读取效率。</p>
</li>
<li><p><strong>C. 减少数据拷贝：</strong> 在 Python 扩展模块与 C++ 交互时，尽量使用零拷贝技术（Zero-Copy）或高效的缓冲区管理，避免 Python 对象和 C++ 结构体之间不必要的频繁数据复制。</p>
</li>
</ol>
</li>
<li><p><strong>总结：</strong> 通过这些底层优化，我的 C++ 实现能够更有效地利用 CPU 资源，尤其是 SIMD 带来的指令级并行，从而获得了额外的 50% 性能提升。</p>
</li>
<li><h4 id="你在开源项目里实现的依赖检测和优先级调度，具体的算法和设计思路是什么？"><a href="#你在开源项目里实现的依赖检测和优先级调度，具体的算法和设计思路是什么？" class="headerlink" title="你在开源项目里实现的依赖检测和优先级调度，具体的算法和设计思路是什么？"></a>你在开源项目里实现的依赖检测和优先级调度，具体的算法和设计思路是什么？</h4></li>
</ol>
<p><strong>正确回答应体现出对复杂调度问题的解决思路：</strong></p>
<blockquote>
<p><strong>回答结构：</strong> 依赖检测 $\to$ 优先级调度 $\to$ 队列管理。</p>
</blockquote>
<ol>
<li><p><strong>依赖检测（拓扑排序）：</strong></p>
<ol>
<li><p><strong>实现思路：</strong> 依赖检测本质上是构建任务之间的<strong>有向无环图 (DAG)<strong>。当用户提交任务时，我首先构建任务图，并计算每个任务的</strong>入度</strong>（即有多少前置依赖任务尚未完成）。</p>
</li>
<li><p><strong>执行逻辑：</strong> 只有当一个任务的入度降为零时（即所有前置任务都已完成），它才会被放入待调度队列。我使用一个<strong>任务依赖管理器</strong>来追踪所有任务的状态和入度，确保只有满足依赖关系的任务才会被释放给调度器。</p>
</li>
</ol>
</li>
<li><p><strong>优先级调度（多级反馈队列）：</strong></p>
</li>
<li><h4 id="🤖你能跟我讲一下你对这个-AI-Infra-的理解吗？"><a href="#🤖你能跟我讲一下你对这个-AI-Infra-的理解吗？" class="headerlink" title="🤖你能跟我讲一下你对这个 AI Infra 的理解吗？"></a>🤖你能跟我讲一下你对这个 AI Infra 的理解吗？</h4></li>
</ol>
<p><strong>正确回答应从核心目标、挑战和技术手段三个层面展开：</strong></p>
<blockquote>
<p>我认为 AI Infrastructure（AI Infra）的核心目标是为大规模 AI 模型（尤其是大语言模型 LLMs）的训练和推理提供<strong>高效、稳定、可扩展</strong>的底层算力、存储和网络支持。</p>
</blockquote>
<ul>
<li>核心目标</li>
</ul>
<ol>
<li><p><strong>提升模型运行速度（速度）：</strong> 优化模型推理和训练的延迟（Latency）和吞吐量（Throughput）。例如，降低 Time-to-First-Token 的时间。</p>
</li>
<li><p><strong>提升模型运行规模（规模）：</strong> 支持更大参数量、更多 GPU 卡的分布式训练和推理，提高算力集群的整体利用率。</p>
</li>
</ol>
<ul>
<li>当前挑战</li>
</ul>
<p>AI Infra 面临的主要挑战在于解决<strong>通信</strong>和<strong>内存</strong>两大瓶颈：</p>
<ol>
<li><p><strong>通信瓶颈：</strong> 在大规模分布式训练中，GPU 间需要频繁进行 All-Reduce 等集体通信操作。解决网络带宽不足和通信延迟是关键。</p>
</li>
<li><p><strong>内存瓶颈：</strong> 大模型的参数、梯度、优化器状态和激活值占据大量显存（HBM）。需要通过各种技术（如异构内存管理、Offloading）来突破单卡显存限制。</p>
</li>
</ol>
<ul>
<li>核心技术手段</li>
</ul>
<p>实现这些目标，主要通过以下几个维度的技术：</p>
<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>维度</td>
<td>技术方向</td>
<td>示例</td>
</tr>
<tr>
<td>分布式训练&#x2F;推理</td>
<td>模型并行、数据并行</td>
<td>Tensor Parallelism, Pipeline Parallelism, DeepSpeed ZeRO 优化器状态分片。</td>
</tr>
<tr>
<td>网络通信优化</td>
<td>高速网络协议&#x2F;硬件</td>
<td>RDMA (RoCE&#x2F;InfiniBand)，NCCL 库优化通信拓扑。</td>
</tr>
<tr>
<td>存储和文件系统</td>
<td>大模型专属存储</td>
<td>优化 I&#x2F;O 路径，解决检查点存储、小文件存储的低效问题，如 3FS。</td>
</tr>
<tr>
<td>系统调度与资源管理</td>
<td>异构资源调度</td>
<td>K8S + Volcano&#x2F;Yunikorn 等，实现 GPU、HBM、网络拓扑感知的高效调度。</td>
</tr>
</tbody></table>
<ol start="6">
<li><h4 id="💾-3FS-具体是什么？它相比传统的分布式文件系统有哪些优化，比如-RDMA-之类的。"><a href="#💾-3FS-具体是什么？它相比传统的分布式文件系统有哪些优化，比如-RDMA-之类的。" class="headerlink" title="💾 3FS 具体是什么？它相比传统的分布式文件系统有哪些优化，比如 RDMA 之类的。"></a>💾 3FS 具体是什么？它相比传统的分布式文件系统有哪些优化，比如 RDMA 之类的。</h4></li>
</ol>
<p><strong>正确回答应清晰区分其客户端和服务端，并重点突出针对大模型的定制优化：</strong></p>
<blockquote>
<p><strong>3FS</strong> (Three Forks File System) 是 DeepSeek 团队为解决大规模 AI&#x2F;LLM 训练和推理中的存储 I&#x2F;O 瓶颈而设计的高性能分布式文件系统。其核心特点是针对大模型工作负载进行了定制化优化。</p>
</blockquote>
<ul>
<li>优化手段对比（相比传统 DFS）</li>
</ul>
<style> td {white-space:nowrap;border:0.5pt solid #dee0e3;font-size:10pt;font-style:normal;font-weight:normal;vertical-align:middle;word-break:normal;word-wrap:normal;}</style>

<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>优化点</td>
<td>传统分布式文件系统（如 HDFS, GlusterFS）</td>
<td>3FS 的定制优化</td>
</tr>
<tr>
<td>I&#x2F;O 路径</td>
<td>依赖内核路径进行 I&#x2F;O，涉及多次内存拷贝（用户态 $\leftrightarrow$ 内核态）。</td>
<td>客户端 FUSE Kernel Bypass (内核旁路)： 通过修改 FUSE 机制，客户端 I&#x2F;O 绕过内核，直接将数据从用户态传输到网络&#x2F;存储，大幅减少内存拷贝次数和 CPU 开销。</td>
</tr>
<tr>
<td>Page Cache</td>
<td>广泛使用内核 Page Cache 来缓存热点数据，适用于通用文件读写。</td>
<td>跳过 Page Cache： 3FS 分析认为大模型 I&#x2F;O 偏向于大规模随机读写，Page Cache 的命中率低且管理开销大。因此，3FS 在客户端选择直接 I&#x2F;O (O_DIRECT) 写入，直接读写磁盘&#x2F;网络，避免 Page Cache 的不必要开销。</td>
</tr>
<tr>
<td>服务端复制</td>
<td>常使用 Quorum 机制或强一致性两阶段提交。</td>
<td>CRCR (Chain-Replicated-Chunky-Read)： 使用链式复制（Chain Replication）进行数据写入，写入者只与链头通信，链头负责同步给链上的节点，优化写入延迟。读取时可以并行从链上的任一节点读取（Chunky Read）。</td>
</tr>
<tr>
<td>网络利用</td>
<td>依赖标准 TCP&#x2F;IP 协议。</td>
<td>原生支持 RDMA (Remote Direct Memory Access)： 3FS 在其数据传输层原生支持 RDMA 协议，实现了 Zero-Copy 和 Kernel-Bypass 的网络传输。这意味着数据可以直接从发送方内存传输到接收方内存，不占用 CPU 资源，极大地降低了网络延迟和提高了吞吐量。</td>
</tr>
</tbody></table>
<ul>
<li><p>结构和 RDMA 细节</p>
<ul>
<li><p><strong>架构分离：</strong> 3FS 采用传统的元数据（Metadata）和数据存储分离架构，元数据通常存储在一致性更好的存储中。</p>
</li>
<li><p><strong>RDMA 的具体作用：</strong> 3FS 利用 RDMA 实现数据传输，它使用的是 <strong>One-Sided RDMA 操作（如</strong> <strong><code>RDMA Write</code></strong> <strong>或</strong> **<code>RDMA Read</code>**<strong>）</strong>，这与传统的 Two-Sided (Send&#x2F;Recv) 不同。</p>
<ol>
<li><strong>优势：</strong> One-Sided 操作不需要目标机器 CPU 的介入（Zero-Copy），也不需要目标进程的显式接收指令，进一步提高了网络传输效率，是实现高吞吐 AI 存储的关键手段。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="四、其他题目"><a href="#四、其他题目" class="headerlink" title="四、其他题目"></a>四、其他题目</h3><ol>
<li><h4 id="Vermeer-作为分布式图计算框架，在处理超大规模图数据（比如亿级节点和边）时，数据分片和跨节点通信是常见的性能瓶颈。你们在项目中是如何优化数据分片策略，或者减少跨节点通信开销的？有没有具体的调优案例可以分享？"><a href="#Vermeer-作为分布式图计算框架，在处理超大规模图数据（比如亿级节点和边）时，数据分片和跨节点通信是常见的性能瓶颈。你们在项目中是如何优化数据分片策略，或者减少跨节点通信开销的？有没有具体的调优案例可以分享？" class="headerlink" title="Vermeer 作为分布式图计算框架，在处理超大规模图数据（比如亿级节点和边）时，数据分片和跨节点通信是常见的性能瓶颈。你们在项目中是如何优化数据分片策略，或者减少跨节点通信开销的？有没有具体的调优案例可以分享？"></a>Vermeer 作为分布式图计算框架，在处理超大规模图数据（比如亿级节点和边）时，数据分片和跨节点通信是常见的性能瓶颈。你们在项目中是如何优化数据分片策略，或者减少跨节点通信开销的？有没有具体的调优案例可以分享？</h4></li>
</ol>
<p>Vermeer（作为 Apache HugeGraph 的分布式图计算框架，也常被称为 HugeGraph-Computer）在处理大规模图数据时，采用了多种分布式系统和图计算的经典策略。</p>
<p>其核心思想是将庞大的图数据分散到多台机器上，通过并行的计算和高效的通信来完成复杂的图分析任务。</p>
<p>以下是 Vermeer 处理大规模数据的几个关键技术：</p>
<ol>
<li>分布式主从（Master-Worker）架构</li>
</ol>
<p>Vermeer 采用一个主节点（Master）和多个计算节点（Worker）的架构：</p>
<ul>
<li><p><strong>Master 节点：</strong> 负责协调整个计算任务。它不参与繁重的计算，主要职责包括：</p>
<ul>
<li><p>接收用户的计算请求。</p>
</li>
<li><p>管理 Worker 节点的状态。</p>
</li>
<li><p>协调数据分区和任务分配。</p>
</li>
<li><p>在计算过程中聚合全局信息（例如，检查所有节点是否都已完成当前迭代）。</p>
</li>
</ul>
</li>
<li><p><strong>Worker 节点：</strong> 负责实际的数据存储和计算。每个 Worker 会分到图的一部分数据，并独立执行分配给它的计算任务。它们消耗大量的 CPU 和内存资源。</p>
</li>
</ul>
<p>这种架构允许系统通过简单地增加 Worker 节点的数量来进行<strong>水平扩展（Scale-out）</strong>，从而处理更大规模的数据集。</p>
<ol start="2">
<li>“内存优先”与自动磁盘溢出（Out-of-Core）</li>
</ol>
<p>Vermeer 被设计为“内存优先”（Memory-First）框架，这意味着它会尽可能将图数据和计算的中间状态加载到内存中，以实现极高的计算性能（官方文档提到许多任务能在秒级到分钟级完成）。</p>
<p>然而，当图数据或消息数据过大，集群总内存无法容纳时，Vermeer <strong>不会</strong>因为内存溢出（OOM）而崩溃。它具备<strong>自动内存管理</strong>能力：</p>
<blockquote>
<p><strong>Vermeer 会自动将内存中容纳不下的数据（例如超大节点的边或消息）溢出（Spill）到磁盘上。</strong></p>
</blockquote>
<p>这个特性至关重要，它使 Vermeer 能够处理<strong>超出集群总内存容量</strong>的超大规模图，尽管在发生磁盘溢出时性能会有所下降，但这保证了计算任务的稳定性和可完成性。</p>
<ol start="3">
<li>基于 Pregel 的 BSP 计算模型</li>
</ol>
<p>Vermeer 实现了 Google Pregel 提出的<strong>BSP（Bulk Synchronous Parallel，体同步并行）</strong>计算模型。这是目前主流分布式图计算框架（如 Giraph）所采用的模型。</p>
<p>在 BSP 模型中，一个图计算任务被分解为一系列的<strong>“超步”（Supersteps）</strong>：</p>
<ol>
<li><p><strong>并行计算：</strong> 在一个超步中，所有 Worker 节点上的所有顶点（Vertex）并行地执行相同的计算逻辑（例如，更新自己的值，或准备发送给邻居的消息）。</p>
</li>
<li><p><strong>消息通信：</strong> 计算完成后，顶点将消息发送给它们的邻居（这些邻居可能在其他机器上）。</p>
</li>
<li><p><strong>全局同步：</strong> Master 节点进行一次全局同步（Barrier），确保所有 Worker 都完成了当前超步的计算和消息发送。</p>
</li>
<li><p><strong>进入下一轮：</strong> 一旦同步完成，系统进入下一个超步，顶点会处理上一轮收到的消息，然后重复“计算-通信-同步”的循环。</p>
</li>
</ol>
<p>这个模型非常适合迭代式的图算法（如 PageRank、社区发现等），它简化了并行程序的设计，并能有效管理分布式通信。</p>
<ol start="4">
<li>图分区策略（Partitioning）</li>
</ol>
<p>为了将一个大图分布到多个 Worker 节点上，必须对其进行“切分”，即<strong>图分区</strong>。这是分布式图计算的核心挑战。</p>
<ul>
<li><p><strong>OLTP 与 OLAP 的区别：</strong> HugeGraph 的<strong>图数据库（OLTP）</strong>层为了快速的读写和插入，主要采用的是<strong>“边切割”（Edge-Cut）</strong>分区。这种方式下，一个顶点只存在于一台机器上，但一条边可能被切开，存储在两台机器上。</p>
</li>
<li><p><strong>Vermeer (OLAP) 的策略：</strong> 对于图分析（OLAP）任务，尤其是符合“幂律分布”（即存在超级节点）的真实世界图，<strong>“点切割”（Vertex-Cut）</strong>通常是更优的策略。</p>
<ul>
<li><p><strong>点切割（Vertex-Cut）：</strong> 一条边只存在于一台机器上，而一个顶点（特别是“超级节点”）可能会被复制成多个“镜像”（Mirrors）分布在多台机器上。</p>
</li>
<li><p><strong>优势：</strong> 这种方式极大地<strong>平衡了计算负载</strong>，避免了单个 Worker 因处理超级节点而成为瓶颈。它将通信开销从“边”转移到了“点”的同步上，更适合迭代计算。</p>
</li>
</ul>
</li>
</ul>
<p>虽然 HugeGraph 的存储层使用 Edge-Cut，但其 OLAP 计算引擎 Vermeer（HugeGraph-Computer）在加载数据进行计算时，会采用更适合 OLAP 任务的<strong>计算分区策略</strong>（如点切割）来组织数据，以实现高效的并行处理。</p>
<p>Vermeer 通过<strong>Master-Worker架构</strong>实现了水平扩展，利用 <strong>BSP 计算模型</strong>来组织迭代计算，采用<strong>内存优先</strong>策略保证速度，并通过<strong>自动磁盘溢出</strong>机制来处理超过内存容量的大数据，最后结合高效的<strong>图分区策略</strong>（如点切割）来解决数据倾斜和通信瓶颈问题。</p>
<p>在 HugeGraph 的官方文档中，关于 Vermeer（HugeGraph-Computer）的<strong>容错机制</strong>（如 Checkpointing，即检查点）的公开资料较少。在传统的 BSP 框架中，通常会通过在超步之间设置检查点来保存计算状态，以便在 Worker 节点失败时能够恢复，但 Vermeer 的具体实现细节并未详细披露。</p>
<ol start="2">
<li><h4 id="Vermeer-如何减少跨节点通信开销？有没有具体的案例？"><a href="#Vermeer-如何减少跨节点通信开销？有没有具体的案例？" class="headerlink" title="Vermeer 如何减少跨节点通信开销？有没有具体的案例？"></a>Vermeer 如何减少跨节点通信开销？有没有具体的案例？</h4></li>
</ol>
<p>这是一个非常核心的分布式图计算问题。跨节点通信（也称为“网络I&#x2F;O”）几乎总是分布式图计算中最大的性能瓶颈。</p>
<p>Vermeer（作为 HugeGraph-Computer 的一部分）以及所有主流的分布式图计算框架（如 GraphX, Giraph, PowerGraph）都致力于解决这个问题。减少通信开销的核心思想是：<strong>尽可能将计算本地化，减少必须跨网络传输的数据量。</strong></p>
<p>以下是几种关键策略以及它们的具体案例：</p>
<ol>
<li>优化的图分区策略 (Graph Partitioning)</li>
</ol>
<p>这是<strong>最重要</strong>的策略。如何将一个大图“切分”并分发到不同的机器上，直接决定了有多少通信量。</p>
<ul>
<li><p><strong>策略：</strong> 目标是最大化“图的局部性”（Locality），即让相互连接紧密的顶点尽可能位于同一台机器上，从而使大量的消息传递发生在机器内部（内存中），而不是跨网络。</p>
</li>
<li><p><strong>反面案例（糟糕的分区）：</strong></p>
<ul>
<li><p><strong>随机&#x2F;哈希分区 (Random&#x2F;Hash Partitioning):</strong> 这是最简单的方法，例如 <code>hash(vertex_ID) % num_workers</code>。</p>
</li>
<li><p><strong>问题：</strong> 真实世界的图（如社交网络、网页图）通常是“幂律分布”的，存在“超级节点”（Supernodes&#x2F;Hubs），例如明星的社交账号。如果使用哈希分区，一个超级节点（如明星A）可能落在 Worker 1 上，而它的 1 亿个粉丝（邻居）被随机分布在所有其他 Worker 上。</p>
</li>
<li><p><strong>通信开销：</strong> 当明星A需要给所有粉丝发消息时（例如在 PageRank 迭代中），Worker 1 必须向集群中几乎所有其他机器发送海量消息，导致 Worker 1 的网络带宽被打满，成为巨大瓶颈。</p>
</li>
</ul>
</li>
<li><p><strong>正面案例（优化的分区）：</strong></p>
<ul>
<li><p><strong>点切割 (Vertex-Cut &#x2F; Edge Partitioning):</strong> 这是 PowerGraph 和 GraphX 等现代框架采用的关键策略，Vermeer 这样的 OLAP 引擎也会采用类似的逻辑。</p>
</li>
<li><p><strong>思路：</strong> 与其让一个节点（Vertex）只属于一台机器（即“边切割”），我们反过来让<strong>一条边（Edge）只属于一台机器</strong>。如果一个顶点（特别是超级节点）连接了分布在多台机器上的边，那么这个顶点就会被“切割”成多个“镜像”（Mirrors）副本。</p>
</li>
<li><p><strong>具体案例：</strong> 还是明星A和她的1亿粉丝。</p>
<ul>
<li><p>我们将1亿条“关注”<strong>边</strong>均匀地分发到所有 Worker 上（例如 100 个 Worker，每个 Worker 负责 100 万条边）。</p>
</li>
<li><p>明星A这个<strong>顶点</strong>，在每个 Worker 上都会有一个“镜像”副本。</p>
</li>
<li><p>当计算需要“明星A”向邻居（粉丝）发送消息时，这个操作会在<strong>所有 100 个 Worker 上并行本地执行</strong>。每个 Worker 上的明星A镜像，只向它本地存储的那 100 万个粉丝顶点发送消息。</p>
</li>
<li><p><strong>开销在哪里？</strong> 通信开销从“一个点对1亿个点”的广播，转移到了“维护明星A的多个镜像副本之间状态一致性”上。通常，只需要在超步（Superstep）结束时，将所有镜像的更新值（例如新的 PageRank 值）聚合一次，再广播回所有镜像即可。这个开销远小于发送1亿条独立消息。</p>
</li>
</ul>
</li>
<li><p><strong>社区发现分区 (Community-based Partitioning):</strong> 在计算开始前，先运行一个轻量级的社区发现算法（如 Louvain），找出图中的紧密集群，然后将同一个集群的顶点划分到同一台机器上。</p>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li>仅激活“活跃”顶点 (Active Vertex Scheduling)</li>
</ol>
<ul>
<li><p><strong>策略：</strong> 在很多图算法中，并非所有顶点在每一轮迭代中都需要计算或发送消息。例如，在广度优先搜索（BFS）或最短路径（SSSP）中，只有当前“前沿”（Frontier）的顶点（即上一轮刚被访问到或刚被更新最短路径的顶点）才需要工作。</p>
</li>
<li><p><strong>具体案例 (SSSP 最短路径):</strong></p>
<ul>
<li><p><strong>低效实现：</strong> 在 BSP 的每个超步中，<strong>所有</strong>顶点都运行计算逻辑，检查收到的消息，如果收到更短的路径就更新自己，并向所有邻居发送自己的新路径。</p>
</li>
<li><p><strong>通信开销：</strong> 在第 5 轮迭代时，可能 99% 的顶点已经找到了它们的最短路径且不再变化，但它们仍然在空转、检查消息、甚至可能重复发送自己不变的路径值，造成了大量无效计算和通信。</p>
</li>
<li><p><strong>高效实现 (Vermeer, Giraph 等):</strong> 框架只维护一个“活跃顶点集”。</p>
<ul>
<li><p>第 0 步：只有源顶点是活跃的。它计算到邻居的距离并发消息。</p>
</li>
<li><p>第 1 步：只有上一轮<strong>收到消息并成功更新了</strong>自己距离的顶点，才会变成“活跃”状态。它们向自己的邻居发送消息。</p>
</li>
<li><p>循环往复。当没有顶点是活跃的时，算法收敛。</p>
</li>
</ul>
</li>
<li><p><strong>效果：</strong> 极大地减少了参与计算和通信的顶点数量，尤其是在算法的后期阶段。</p>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li>消息合并与批量发送 (Message Combining &#x2F; Aggregation)</li>
</ol>
<ul>
<li><p><strong>策略：</strong> 发送大量小数据包的网络开销（协议握手、路由等）远大于发送一个大数据包。因此，框架应避免在顶点之间进行点对点的零散通信。</p>
</li>
<li><p><strong>具体案例 (PageRank):</strong></p>
<ul>
<li><p><strong>低效实现：</strong> Worker 1 上的 1000 个顶点，分别需要向 Worker 2 上的 5000 个不同顶点发送 PageRank 贡献值。这可能导致 Worker 1 和 Worker 2 之间产生 $1000 \times N$ （N是邻居数）次微小的网络传输。</p>
</li>
<li><p><strong>高效实现 (Vermeer 的 BSP 模型):</strong></p>
<ul>
<li><p>在 BSP 的计算阶段，Worker 1 上的顶点只是在<strong>本地内存</strong>中“投票”或“准备消息”，说“我要给 Worker 2 上的 V_id&#x3D;123 发送 0.5”。</p>
</li>
<li><p>所有这些消息都被暂存在 Worker 1 的一个“出站缓冲区”（Outgoing Buffer）中，按目标机器（Worker 2）进行分组。</p>
</li>
<li><p>在 BSP 的通信阶段（同步之前），Worker 1 将发往 Worker 2 的<strong>所有</strong>消息（可能成千上万条）打包成一个或几个大的数据块，<strong>一次性</strong>批量发送给 Worker 2。</p>
</li>
<li><p>Worker 2 收到这个大包后，在本地将其拆分，分发给相应的目标顶点。</p>
</li>
</ul>
</li>
<li><p><strong>效果：</strong> 将数百万次的“微小”通信合并为（Worker数量 $\times$ Worker数量）次“批量”通信。</p>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li>采用 GAS 计算模型 (Gather-Apply-Scatter)</li>
</ol>
<p>这是对“以顶点为中心”模型（Vermeer 所基于的 Pregel 模型）的一种演进，在 PowerGraph 和 GraphX 中被明确提出，它能更精细地控制通信。</p>
<ul>
<li><p><strong>策略：</strong> 将一个超步分解为三个阶段：</p>
<ul>
<li><p><strong>Gather (收集):</strong> 顶点从它的邻居那里“拉取”（Pull）或收集信息。</p>
</li>
<li><p><strong>Apply (应用):</strong> 顶点使用收集到的信息，更新自己的状态。</p>
</li>
<li><p><strong>Scatter (分散):</strong> 顶点将自己的新状态“推送”（Push）给邻居（如果需要的话）。</p>
</li>
</ul>
</li>
<li><p><strong>具体案例 (结合点切割):</strong></p>
<ul>
<li><p>在点切割（Vertex-Cut）分区中，一个顶点 V 被复制到了多台机器上（$V_{m1}$, $V_{m2}$…）。</p>
</li>
<li><p><strong>Gather 阶段：</strong> 每个镜像 $V_{m1}$ 只从它在 Worker 1 上的<strong>本地</strong>邻居收集信息，计算出一个“部分和”。</p>
</li>
<li><p><strong>通信发生：</strong> 所有镜像（$V_{m1}$, $V_{m2}$…）将自己的“部分和”发送给一个指定的“主顶点”（Master Vertex）。</p>
</li>
<li><p><strong>Apply 阶段：</strong> “主顶点”将所有部分和相加，更新 V 的最终状态（例如，新的 PageRank 值）。</p>
</li>
<li><p><strong>Scatter 阶段（通信）：</strong> “主顶点”将这个<strong>唯一</strong>的最终值广播回给所有镜像（$V_{m1}$, $V_{m2}$…），以便它们在下一轮迭代中使用。</p>
</li>
</ul>
</li>
<li><p><strong>效果：</strong> GAS 模型将通信集中在 Gather 和 Scatter 阶段，并且与点切割策略完美配合，将超级节点的通信开销从 $O(N)$（N为邻居数）降低到 $O(M)$（M为镜像数，即机器数），这是一个巨大的优化。</p>
</li>
</ul>
<hr>
<p><strong>总结：</strong> Vermeer 这样的分布式图计算框架，其性能在很大程度上取决于它如何智能地<strong>分区</strong>图（特别是如何处理超级节点）以及如何<strong>调度</strong>计算（如只调度活跃顶点），并<strong>批量化</strong>和<strong>结构化</strong>（如 BSP 或 GAS 模型）地组织通信。</p>
<ol start="3">
<li><h4 id="在百度-HugeGraph-项目中，你提到-“调度器优先级调度和依赖检测”，假设现在有一个超大规模图计算任务，需要处理千万级节点的遍历，且每个节点处理依赖多个远程服务调用。在这种高并发场景下，你会如何设计调度策略以避免任务阻塞？是否考虑过引入异步处理或分布式锁机制？具体如何实现？"><a href="#在百度-HugeGraph-项目中，你提到-“调度器优先级调度和依赖检测”，假设现在有一个超大规模图计算任务，需要处理千万级节点的遍历，且每个节点处理依赖多个远程服务调用。在这种高并发场景下，你会如何设计调度策略以避免任务阻塞？是否考虑过引入异步处理或分布式锁机制？具体如何实现？" class="headerlink" title="在百度 HugeGraph 项目中，你提到 “调度器优先级调度和依赖检测”，假设现在有一个超大规模图计算任务，需要处理千万级节点的遍历，且每个节点处理依赖多个远程服务调用。在这种高并发场景下，你会如何设计调度策略以避免任务阻塞？是否考虑过引入异步处理或分布式锁机制？具体如何实现？"></a>在百度 HugeGraph 项目中，你提到 “调度器优先级调度和依赖检测”，假设现在有一个超大规模图计算任务，需要处理千万级节点的遍历，且每个节点处理依赖多个远程服务调用。在这种高并发场景下，你会如何设计调度策略以避免任务阻塞？是否考虑过引入异步处理或分布式锁机制？具体如何实现？</h4></li>
<li><h4 id="在湖杉科技的工作中，你用-LoRa-优化大模型提升了-20-用户满意度。假设现在需要将一个微调后的大模型部署到生产环境，且用户对延迟敏感（如实时推荐系统），你会如何平衡模型精度和推理速度？比如是否会考虑模型量化、剪枝或硬件加速？具体技术方案是什么？"><a href="#在湖杉科技的工作中，你用-LoRa-优化大模型提升了-20-用户满意度。假设现在需要将一个微调后的大模型部署到生产环境，且用户对延迟敏感（如实时推荐系统），你会如何平衡模型精度和推理速度？比如是否会考虑模型量化、剪枝或硬件加速？具体技术方案是什么？" class="headerlink" title="在湖杉科技的工作中，你用 LoRa 优化大模型提升了 20% 用户满意度。假设现在需要将一个微调后的大模型部署到生产环境，且用户对延迟敏感（如实时推荐系统），你会如何平衡模型精度和推理速度？比如是否会考虑模型量化、剪枝或硬件加速？具体技术方案是什么？"></a>在湖杉科技的工作中，你用 LoRa 优化大模型提升了 20% 用户满意度。假设现在需要将一个微调后的大模型部署到生产环境，且用户对延迟敏感（如实时推荐系统），你会如何平衡模型精度和推理速度？比如是否会考虑模型量化、剪枝或硬件加速？具体技术方案是什么？</h4></li>
<li><h4 id="在动态软件更新框架项目中，你设计了基于-PLC-周期的更新机制。如果在分布式系统中，部分节点更新成功而另一部分失败，导致系统状态不一致，你会如何设计全局回滚策略？是否考虑过引入事务性更新或一致性协议（如-Paxos、Raft）？"><a href="#在动态软件更新框架项目中，你设计了基于-PLC-周期的更新机制。如果在分布式系统中，部分节点更新成功而另一部分失败，导致系统状态不一致，你会如何设计全局回滚策略？是否考虑过引入事务性更新或一致性协议（如-Paxos、Raft）？" class="headerlink" title="在动态软件更新框架项目中，你设计了基于 PLC 周期的更新机制。如果在分布式系统中，部分节点更新成功而另一部分失败，导致系统状态不一致，你会如何设计全局回滚策略？是否考虑过引入事务性更新或一致性协议（如 Paxos、Raft）？"></a>在动态软件更新框架项目中，你设计了基于 PLC 周期的更新机制。如果在分布式系统中，部分节点更新成功而另一部分失败，导致系统状态不一致，你会如何设计全局回滚策略？是否考虑过引入事务性更新或一致性协议（如 Paxos、Raft）？</h4></li>
<li><h4 id="你的项目涉及-C-、Python、Go-等多语言开发。在团队协作中，如何确保不同语言模块之间的接口兼容性和代码质量？例如，是否采用接口定义语言（IDL）或自动化测试框架？有没有遇到过因语言特性差异导致的线上问题？如何解决的？"><a href="#你的项目涉及-C-、Python、Go-等多语言开发。在团队协作中，如何确保不同语言模块之间的接口兼容性和代码质量？例如，是否采用接口定义语言（IDL）或自动化测试框架？有没有遇到过因语言特性差异导致的线上问题？如何解决的？" class="headerlink" title="你的项目涉及 C++、Python、Go 等多语言开发。在团队协作中，如何确保不同语言模块之间的接口兼容性和代码质量？例如，是否采用接口定义语言（IDL）或自动化测试框架？有没有遇到过因语言特性差异导致的线上问题？如何解决的？"></a>你的项目涉及 C++、Python、Go 等多语言开发。在团队协作中，如何确保不同语言模块之间的接口兼容性和代码质量？例如，是否采用接口定义语言（IDL）或自动化测试框架？有没有遇到过因语言特性差异导致的线上问题？如何解决的？</h4></li>
<li><h4 id="在九维数智的无锁数据结构开发中，你提到-“解决性能瓶颈”。假设现在有一个基于无锁队列的生产者-消费者系统，消费者处理速度突然下降，但队列无阻塞，你会如何定位问题？是否会使用性能分析工具（如-perf、Valgrind）？具体步骤是什么？"><a href="#在九维数智的无锁数据结构开发中，你提到-“解决性能瓶颈”。假设现在有一个基于无锁队列的生产者-消费者系统，消费者处理速度突然下降，但队列无阻塞，你会如何定位问题？是否会使用性能分析工具（如-perf、Valgrind）？具体步骤是什么？" class="headerlink" title="在九维数智的无锁数据结构开发中，你提到 “解决性能瓶颈”。假设现在有一个基于无锁队列的生产者 - 消费者系统，消费者处理速度突然下降，但队列无阻塞，你会如何定位问题？是否会使用性能分析工具（如 perf、Valgrind）？具体步骤是什么？"></a>在九维数智的无锁数据结构开发中，你提到 “解决性能瓶颈”。假设现在有一个基于无锁队列的生产者 - 消费者系统，消费者处理速度突然下降，但队列无阻塞，你会如何定位问题？是否会使用性能分析工具（如 perf、Valgrind）？具体步骤是什么？</h4></li>
<li><h4 id="在-P-D-动态拆分合并的推理引擎系统中，你基于-vllm-做了-Prefill-和-Decode-的性能建模。如果现在要设计一个全新的推理引擎，你会选择自研还是基于现有框架（如-vllm、TGI）？为什么？在架构设计上会重点关注哪些技术点（如内存管理、并行计算）？"><a href="#在-P-D-动态拆分合并的推理引擎系统中，你基于-vllm-做了-Prefill-和-Decode-的性能建模。如果现在要设计一个全新的推理引擎，你会选择自研还是基于现有框架（如-vllm、TGI）？为什么？在架构设计上会重点关注哪些技术点（如内存管理、并行计算）？" class="headerlink" title="在 P-D 动态拆分合并的推理引擎系统中，你基于 vllm 做了 Prefill 和 Decode 的性能建模。如果现在要设计一个全新的推理引擎，你会选择自研还是基于现有框架（如 vllm、TGI）？为什么？在架构设计上会重点关注哪些技术点（如内存管理、并行计算）？"></a>在 P-D 动态拆分合并的推理引擎系统中，你基于 vllm 做了 Prefill 和 Decode 的性能建模。如果现在要设计一个全新的推理引擎，你会选择自研还是基于现有框架（如 vllm、TGI）？为什么？在架构设计上会重点关注哪些技术点（如内存管理、并行计算）？</h4></li>
</ol>

            
        </div>
        <div class="page-template-comments">
            
        </div>
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Ethereal</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.2.1</a>
        </div>
        
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>





    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts pjax">
    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
